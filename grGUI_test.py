import torch
from peft import PeftModel
from transformers import (
    ColPaliForRetrieval,
    ColPaliProcessor,
    AutoProcessor,
    Qwen2_5_VLForConditionalGeneration,
)
import gradio as gr
from PIL import Image
from torch.utils.data import DataLoader
from tqdm import tqdm
from qwen_vl_utils import process_vision_info
from pdf2image import convert_from_path

# =======================================
# 1. å…¨å±€åŠ è½½ï¼šåªè¿è¡Œä¸€æ¬¡ï¼Œå¹¶æ”¾åœ¨ CUDA
# =======================================
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# (1) ColPali æ£€ç´¢æ¨¡å‹ + Adapter
BASE_MODEL_PATH = "/home/linux/yyj/colpali/finetune/colpali-v1.2-hf"
ADAPTER_PATH    = "/home/linux/yyj/colpali/finetune/wiki_city"

colpali_base = ColPaliForRetrieval.from_pretrained(
    BASE_MODEL_PATH,
    torch_dtype=torch.bfloat16,
    device_map={"": DEVICE}
)
colpali_model = PeftModel.from_pretrained(colpali_base, ADAPTER_PATH)
colpali_model.to(DEVICE)
colpali_processor = ColPaliProcessor.from_pretrained(ADAPTER_PATH)

# (2) Qwen2.5-VL æ–‡å›¾ç”Ÿæˆæ¨¡å‹
QWEN_DIR   = "/home/linux/yyj/colpali/finetune/Qwen2.5-VL-3B-Instruct"
qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    QWEN_DIR,
    torch_dtype=torch.bfloat16,
    device_map={"": DEVICE}
)
qwen_model.to(DEVICE)
qwen_processor = AutoProcessor.from_pretrained(QWEN_DIR, max_pixels=1280*28*28)

# =======================================
# 2. ç´¢å¼•å‡½æ•°ï¼šæ‰¹é‡åŒ– & å…¨éƒ¨æ”¾ CUDA
# =======================================
def index(file_list, ds):
    images = []
    for f in file_list:
        images.extend(convert_from_path(f))
    loader = DataLoader(
        images, batch_size=8, shuffle=False,
        collate_fn=lambda imgs: colpali_processor.process_images(imgs)
    )
    for batch in tqdm(loader, desc="ç´¢å¼•é¡µé¢"):
        with torch.no_grad():
            batch = {k: v.to(DEVICE) for k, v in batch.items()}
            out = colpali_model(**batch)
            embs = out.embeddings  # b x seq_len x dim on DEVICE
            ds.extend(torch.unbind(embs))
    return f"å·²ä¸Šä¼ å¹¶ç´¢å¼• {len(images)} é¡µ", ds, images

# =======================================
# 3. æ£€ç´¢å‡½æ•°ï¼šå• query æ‰¹é‡ & CUDA
# =======================================
def search(query: str, ds, images):
    with torch.no_grad():
        q_inputs = colpali_processor.process_queries([query])
        q_inputs = {k: v.to(DEVICE) for k, v in q_inputs.items()}
        out = colpali_model(**q_inputs)
        q_embs = out.embeddings  # 1 x seq_len x dim on DEVICE
        qs = list(torch.unbind(q_embs))
    scores = colpali_processor.score_retrieval(qs, ds)
    best_idx = int(scores.argmax())
    return f"æœ€ç›¸å…³çš„é¡µé¢æ˜¯ {best_idx}", images[best_idx]

# =======================================
# 4. å¤šæ¨¡æ€é—®ç­”ï¼šå…¨éƒ¨ CUDA
# =======================================
FIXED_PROMPT = (
    "è¯·è¯¦ç»†åˆ†æè¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š\n"
    "1. å›¾ç‰‡çš„ä¸»è¦å†…å®¹å’Œä¸»é¢˜\n"
    "2. å›¾ç‰‡ä¸­çš„å…³é”®ä¿¡æ¯ç‚¹\n"
    "3. å›¾ç‰‡çš„å¸ƒå±€å’Œç»“æ„\n"
    "4. å›¾ç‰‡ä¸­å¯èƒ½åŒ…å«çš„é‡è¦æ•°æ®æˆ–ç»Ÿè®¡ä¿¡æ¯\n"
    "è¯·ä»¥ç»“æ„åŒ–çš„æ–¹å¼è¾“å‡ºï¼Œç¡®ä¿ä¿¡æ¯æ¸…æ™°æ˜“è¯»ã€‚"
)

def get_answer_qwen25vl(prompt: str, image: Image.Image):
    messages = [
        {"role": "user", "content": [
            {"type": "image", "image": image},
            {"type": "text",  "text": prompt},
        ]}
    ]
    text = qwen_processor.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    img_inputs, vid_inputs = process_vision_info(messages)
    inputs = qwen_processor(
        text=[text],
        images=img_inputs,
        videos=vid_inputs,
        padding=True,
        return_tensors="pt",
    )
    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}

    with torch.no_grad():
        output_ids = qwen_model.generate(**inputs, max_new_tokens=512)
    trimmed = [out[len(inp):] for inp, out in zip(inputs['input_ids'], output_ids)]
    return qwen_processor.batch_decode(
        trimmed,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )[0]

# =======================================
# 5. ç»„åˆå‡½æ•°
# =======================================
def search_with_llm(query, ds, images):
    msg, best_img = search(query, ds, images)
    answer = get_answer_qwen25vl(query, best_img)
    return msg, best_img, answer

# =======================================
# 6. Gradio ç•Œé¢é…ç½®
# =======================================
with gr.Blocks() as demo:
    gr.Markdown("# ColPali æ–‡æ¡£ + å¤šæ¨¡æ€é—®ç­”ç³»ç»Ÿ ğŸ“šğŸ”")
    file    = gr.File(file_types=[".pdf"], file_count="multiple", label="ä¸Šä¼  PDF")
    btn_idx = gr.Button("è½¬æ¢å¹¶ç´¢å¼•")
    out_msg = gr.Textbox(label="çŠ¶æ€")
    state_ds  = gr.State([])
    state_imgs= gr.State([])

    btn_idx.click(
        index, inputs=[file, state_ds], outputs=[out_msg, state_ds, state_imgs]
    )

    qry    = gr.Textbox(placeholder="è¾“å…¥æŸ¥è¯¢", label="æŸ¥è¯¢æ–‡æœ¬")
    btn_s  = gr.Button("æœç´¢")
    out_msg2 = gr.Textbox(label="æ£€ç´¢ç»“æœ")
    out_img  = gr.Image(label="æœ€ä½³é¡µé¢")
    out_txt  = gr.Textbox(label="æ¨¡å‹å›ç­”")

    btn_s.click(
        search_with_llm,
        inputs=[qry, state_ds, state_imgs],
        outputs=[out_msg2, out_img, out_txt]
    )

if __name__ == "__main__":
    demo.queue(max_size=10).launch(debug=True, share=True)
